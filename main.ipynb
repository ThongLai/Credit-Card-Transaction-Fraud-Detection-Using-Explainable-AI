{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ThongLai/Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI/main?urlpath=%2Fdoc%2Ftree%2Fmain.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'architectures/'\n",
    "DATASET_PATH = 'dataset/'\n",
    "RANDOM_SEED = 42 # Set to `None` for the generator uses the current system time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on `Binder`, then it is no need to set up the packages again\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# ---OR---\n",
    "\n",
    "# %pip install tensorflow==2.10.1 numpy==1.26.4 pandas scikit-learn imblearn matplotlib seaborn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: `3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]`\n",
      "Base Python location: `C:\\Users\\LMT\\AppData\\Local\\Programs\\Python\\Python310`\n",
      "Current Environment location: `.venv_xai_fraud_detection`\n",
      "\n",
      "Tensorflow version: `2.10.1`\n",
      "CUDNN version: `64_8`\n",
      "CUDA version: `64_112`\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from anchor import anchor_tabular\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import test dataset and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.kaggle.com/api/v1/datasets/download/kartik2112/fraud-detection/fraudTest.csv\n",
      "File `dataset/fraudTest.csv` already exists.\n",
      "URL: https://www.kaggle.com/api/v1/datasets/download/kartik2112/fraud-detection/fraudTrain.csv\n",
      "File `dataset/fraudTrain.csv` already exists.\n"
     ]
    }
   ],
   "source": [
    "utils.download_dataset_from_kaggle('fraudTest.csv')\n",
    "utils.download_dataset_from_kaggle('fraudTrain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(os.path.join(DATASET_PATH, 'fraudTrain.csv'), index_col=0)\n",
    "data_test = pd.read_csv(os.path.join(DATASET_PATH, 'fraudTest.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal-Encoding is applied for `['merchant', 'category', 'gender', 'age_group']`\n",
      "SMOTE is applied\n",
      "Ordinal-Encoding is applied for `['merchant', 'category', 'gender', 'age_group']`\n",
      "SMOTE is applied\n"
     ]
    }
   ],
   "source": [
    "data_train = utils.feature_engineering(data_train)\n",
    "X_train, y_train, data_train, transformations = utils.pre_processing(data_train)\n",
    "\n",
    "data_test = utils.feature_engineering(data_test)\n",
    "X_test, y_test, data_test, transformations = utils.pre_processing(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pre-trained models and store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== MODEL METADATA =====\n",
      "\n",
      "\n",
      "=== Model: `model_1_Siddhartha_CNN_acc99` ===\n",
      "Input shape: (None, 13, 1)\n",
      "Output shape: (None, 1)\n",
      "Number of layers: 9\n",
      "Total parameters: 51,873\n",
      "File size: 0.00 MB\n",
      "Last modified: Thu Apr 10 16:24:44 2025\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = utils.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19133/34599 [===============>..............] - ETA: 7:50"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for model_name, model in models.items():\n",
    "    y_predict = model.predict(X_test)\n",
    "    y_predict_binary = np.round(y_predict).astype(int).squeeze()\n",
    "    predictions[model_name] = y_predict_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Credit Card Fraud Dataset Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field Name | Description |\n",
    "|------------|-------------|\n",
    "| **trans_date_trans_time** | Date and time when transaction occurred |\n",
    "| **cc_num** | Credit card number of customer |\n",
    "| **merchant** | Name of merchant where transaction occurred |\n",
    "| **category** | Category of merchant (e.g., retail, food, etc.) |\n",
    "| **amt** | Amount of transaction |\n",
    "| **first** | First name of credit card holder |\n",
    "| **last** | Last name of credit card holder |\n",
    "| **gender** | Gender of credit card holder |\n",
    "| **street** | Street address of credit card holder |\n",
    "| **city** | City of credit card holder |\n",
    "| **state** | State of credit card holder |\n",
    "| **zip** | ZIP code of credit card holder |\n",
    "| **lat** | Latitude location of credit card holder |\n",
    "| **long** | Longitude location of credit card holder |\n",
    "| **city_pop** | Population of credit card holder's city |\n",
    "| **job** | Occupation of credit card holder |\n",
    "| **dob** | Date of birth of credit card holder |\n",
    "| **trans_num** | Transaction number |\n",
    "| **unix_time** | UNIX timestamp of transaction |\n",
    "| **merch_lat** | Latitude location of merchant |\n",
    "| **merch_long** | Longitude location of merchant |\n",
    "| **is_fraud** | Target class indicating whether transaction is fraudulent (1) or legitimate (0) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.10.1 numpy==1.26.4 pandas scikit-learn imblearn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKeyM24MXCM5"
   },
   "source": [
    "# XAI Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13688,
     "status": "ok",
     "timestamp": 1742934071114,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "2HaMQx0dbHzL",
    "outputId": "44e8f378-2799-45fa-ef29-f04bd44a896a"
   },
   "outputs": [],
   "source": [
    "# %pip install shap lime anchor-exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LIME and Anchors, we need to define some characteristics of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features\n",
    "features = data_test.columns.drop('is_fraud')\n",
    "\n",
    "# Collect all categorical features\n",
    "categorical_features = list(data_test.select_dtypes(include=['bool', 'category', 'object']).columns)\n",
    "\n",
    "# Collect all misclassified entries (For later explaination on why the model predicted them incorrectly)\n",
    "misclassified_indices = np.where(y_test != y_predict_binary)[0]\n",
    "print(f\"Found {len(misclassified_indices)} misclassified instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOYJJGKJXCM6"
   },
   "source": [
    "## SHAP 🎲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBPavCPaXCM6"
   },
   "source": [
    "### Get SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DeepExplainer`(specifcifically for neural networks)\n",
    "\n",
    "**For `DeepExplainer`, we need to create a `background` dataset**: This is because deep neural networks are complex and non-linear, so they require reference points (background samples) to understand how the model normally behaves and accurately calculate feature importance.\n",
    "\n",
    "Passing the entire training dataset as data will give very accurate expected values, but be unreasonably expensive. The variance of the expectation estimates scale by roughly `1/sqrt(N)` for `N` background data samples.\n",
    "\n",
    "So 100 samples will give a good estimate, and 1000 samples a very good estimate of the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742934211210,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "pGf_Bvq9XCM6"
   },
   "outputs": [],
   "source": [
    "def SHAP(model, X_train, X_test, from_idx, to_idx, background_size=100):\n",
    "    X_train = np.expand_dims(X_train, axis=-1) if X_train.shape[-1] != 1 else X_train\n",
    "    X_test = np.expand_dims(X_test, axis=-1) if X_test.shape[-1] != 1 else X_test\n",
    "\n",
    "    background = X_train[np.random.choice(len(X_train), background_size, replace=False)]\n",
    "\n",
    "    explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test[from_idx:to_idx+1]) # Deep learning models expect 2D input arrays (samples × features), X_test[idx] only returns a 1D array (shape: (n_features,)\n",
    "\n",
    "    shap_values = shap_values.squeeze()[np.newaxis, ...] if shap_values.shape[0] == 1 else shap_values.squeeze()\n",
    "\n",
    "    return explainer, shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9_o30OgXCM6"
   },
   "source": [
    "### Global Interpretability (whole test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "error",
     "timestamp": 1742934213171,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "lGaL7LjtXCM7",
    "outputId": "0edda4ac-8f9d-4cda-dbfa-ae894882047d"
   },
   "outputs": [],
   "source": [
    "# Call the function to obtain SHAP values.\n",
    "from_idx = misclassified_indices[0]\n",
    "to_idx = misclassified_indices[0]\n",
    "# to_idx = len(X_test)-1\n",
    "\n",
    "explainer, shap_values = SHAP(model, X_train, X_test, from_idx, to_idx, background_size=100)\n",
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95704,
     "status": "aborted",
     "timestamp": 1742934046661,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "rFIzKUE7XCM7"
   },
   "outputs": [],
   "source": [
    "def get_top_n_features(shap_values, features, n=10):\n",
    "    mean_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    df_shap = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'mean_abs_shap': np.squeeze(mean_shap_values)\n",
    "    }).set_index('feature')\n",
    "\n",
    "    df_shap = df_shap.reindex(df_shap['mean_abs_shap'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "    # Get top n features\n",
    "    n = 10\n",
    "    top_n_features = list(df_shap.head(n).index)\n",
    "\n",
    "    display(df_shap.head(n))\n",
    "\n",
    "    return top_n_features\n",
    "\n",
    "top_n_features = get_top_n_features(shap_values, features, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a496_dzwXCM7"
   },
   "source": [
    "### Visualization\n",
    "[SHAP Plots Explained](https://www.youtube.com/playlist?list=PLpoCVQU4m6j9HDOzRBL4nX4eol9DrZ3Kd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFdpTO9CXCM7"
   },
   "source": [
    "#### Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95709,
     "status": "aborted",
     "timestamp": 1742934046668,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "VeMFhKzCXCM8"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neoV_RfTXCM8"
   },
   "source": [
    "#### Force Plot\n",
    "\n",
    "[How to use Shapley Additive Explanations for Black Box Machine Learning Algorithms](https://www.youtube.com/watch?v=7wnG6Wnm2uU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95708,
     "status": "aborted",
     "timestamp": 1742934046670,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "FG_GjhTWXCM8"
   },
   "outputs": [],
   "source": [
    "# Plot feature contributions for a prediction\n",
    "shap.initjs()\n",
    "baseline = explainer.expected_value.numpy()\n",
    "\n",
    "shap.force_plot(baseline, shap_values, data_test.loc[y_test.index].drop('is_fraud', axis=1).iloc[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME 🍋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local interpretable model-agnostic explanations (LIME) [[Paper, 2016]](https://arxiv.org/abs/1602.04938)\n",
    "\n",
    "> *Interpretable models that are used to explain individual predictions of black box machine learning models (for credit card fraud detection in this project)*\n",
    "\n",
    "#### **LIME Process in Fraud Detection:**\n",
    "1. **Select** a transaction of interest (potential fraud case)\n",
    "2. **Perturb** the transaction features and get black box predictions for these perturbed samples\n",
    "3. **Generate** a new dataset consisting of perturbed transaction samples and corresponding fraud/legitimate predictions\n",
    "4. **Train** an interpretable model, weighted by proximity of sampled transactions to the transaction of interest\n",
    "5. **Interpret** the local model to explain why a specific transaction was flagged as fraudulent\n",
    "\n",
    "#### **Technical Implementation Details**\n",
    "For credit card fraud detection, tabular explainers need a training set of transactions . This is because statistics are computed on each feature (such as transaction amount, location, time of day, etc.):\n",
    "* **Numerical features** (like transaction amount): compute mean and std, discretize into quartiles\n",
    "* **Categorical features** (like merchant type): compute frequency of each value\n",
    "\n",
    "This scaling allows meaningful computation of distances between transactions when attributes are not on the same scale.\n",
    "\n",
    "#### **Key Parameters**\n",
    "LIME uses an exponential smoothing kernel to define the neighborhood of similar transactions:\n",
    "* **Small kernel width** = only very similar transactions influence the local model (high precision)\n",
    "* **Larger kernel width** = transactions with more differences can also influence the model (higher coverage)\n",
    "\n",
    "When applied to credit card fraud detection, LIME can help explain why a specific transaction was flagged as suspicious by your neural network or other black-box model, making the detection process transparent to financial stakeholders and customers.\n",
    "\n",
    "[LIME Code Tutorial from original paper authors](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Original Transaction Instance**\n",
    "\n",
    "```python\n",
    "transaction = {\n",
    "    'amt': 1850.75,        # Transaction amount\n",
    "    'age': 27,             # Customer age\n",
    "    'dist': 792.3,         # Distance from home location\n",
    "    'F': 1,                # Female gender\n",
    "    'M': 0,                # Male gender\n",
    "    '20 to 30': 1,         # Age bracket\n",
    "    'AK': 1,               # State (Alaska)\n",
    "    'shopping_pos': 1,     # Transaction category\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 1: Generate Perturbations**\n",
    "\n",
    "Creating slightly modified versions (perturbations) of the original transaction by adding small random noise based on the feature's statistics (mean and standard deviation):\n",
    "\n",
    "```python\n",
    "perturbed_transactions = [\n",
    "    {    # Perturbed 1 (`AK` changed, `CA` added)\n",
    "        'amt': 1750.25, 'age': 27, 'dist': 792.3, \n",
    "        'F': 1, 'M': 0, '20 to 30': 1, \n",
    "        'AK': 0, 'CA': 1, 'shopping_pos': 1\n",
    "    },\n",
    "    {    # Perturbed 2 (`shopping_pos` `changed`, `grocery_pos` added)\n",
    "        'amt': 1850.75, 'age': 27, 'dist': 792.3,\n",
    "        'F': 1, 'M': 0, '20 to 30': 1, \n",
    "        'AK': 1, 'shopping_pos': 0, 'grocery_pos': 1\n",
    "    },\n",
    "    {    # Perturbed 3 (`distance`, `age` changed)\n",
    "        'amt': 1850.75, 'age': 42, 'dist': 156.7,\n",
    "        'F': 1, 'M': 0, '20 to 30': 0, '40 to 50': 1, \n",
    "        'AK': 1, 'shopping_pos': 1\n",
    "    },\n",
    "    # ... many more variations\n",
    "]\n",
    "\n",
    "# Get predictions from the black box model\n",
    "predictions = [\n",
    "    0.35,  # Transaction 1 - lower probability of fraud\n",
    "    0.42,  # Transaction 2\n",
    "    0.28,  # Transaction 3\n",
    "    # ... and so on\n",
    "]\n",
    "```\n",
    "\n",
    "#### **Step 2: Analyze Feature Distributions**\n",
    "LIME analyzes the distribution of values in the perturbed samples:\n",
    "```python\n",
    "# Feature statistics for discretization\n",
    "amt_stats = {\n",
    "    'mean': 1523.45,\n",
    "    'std': 342.87,\n",
    "    'thresholds': [-0.60, -0.46, -0.32, -0.18, 0.04, 0.26, 0.48]  # normalized\n",
    "}\n",
    "\n",
    "dist_stats = {\n",
    "    'mean': 457.23,\n",
    "    'std': 389.52,\n",
    "    'thresholds': [-0.82, -0.51, -0.20, 0.09, 0.41, 0.75, 1.08]  # normalized\n",
    "}\n",
    "\n",
    "# ...\n",
    "\n",
    "For binary categorical features thresholds, LIME typically uses: The percent point function (ppf) of the normal distribution + A small adjustment factor (typically around 0.4 to 0.6) + A small shift constant (often between -0.1 and 0.1)\n",
    "\n",
    "# Categorical feature analysis\n",
    "AK_stats = {\n",
    "    'frequency': 0.03,\n",
    "    'ppf_calculation': -1.88,  # (ppf(0.03) ≈ -1.88) Inverse of standard normal CDF at 0.03 \n",
    "    'threshold': -0.06,  # -1.88 * 0.03 - 0.00 ≈ -0.06 (Low adjustment factor (0.03) used due to feature rarity; no shift needed)\n",
    "}\n",
    "\n",
    "shopping_pos_stats = {\n",
    "    'frequency': 0.15,\n",
    "    'ppf_calculation': -1.04, # (ppf(0.15) ≈ -1.04) Inverse of standard normal CDF at 0.15\n",
    "    'threshold': -0.33,  # -1.04 * 0.3 + 0.0 ≈ -0.312 (Medium adjustment factor (0.3); no shift applied as base calculation was close to desired scale)\n",
    "}\n",
    "\n",
    "grocery_pos_stats = {\n",
    "    'frequency': 0.22,\n",
    "    'ppf_calculation': -0.77, # (ppf(0.22) ≈ -0.77) Inverse of standard normal CDF at 0.22\n",
    "    'threshold': -0.43,  # -0.77 * 0.45 - 0.08 ≈ -0.427 (Medium-high adjustment (0.45) with small negative shift to maintain consistent relationship with `shopping_pos`)\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 3: Discretize Continuous Features**\n",
    "LIME converts continuous features into binary features using thresholds:\n",
    "```python\n",
    "# Original transaction (normalized)\n",
    "normalized_transaction = {\n",
    "    'amt': -0.51,  # (1850.75 - mean) / std\n",
    "    'dist': 0.39,  # (792.3 - mean) / std\n",
    "    # ...other features\n",
    "}\n",
    "\n",
    "# Binary features after discretization\n",
    "binary_features = {\n",
    "    '-0.60 < amt <= -0.46': 1,  # True\n",
    "    '0.09 < dist <= 0.41': 1,   # True\n",
    "    'AK <= -0.06': 1,                    # True\n",
    "    'F <= 0.91': 1,                     # True\n",
    "    'shopping_pos <= -0.33': 1,          # True\n",
    "    # ...\n",
    "    'amt <= -0.60': 0, \n",
    "    '-0.60 < amt <= -0.46': 0, \n",
    "    '-0.46 < amt <= -0.32': 0,\n",
    "    '-0.32 < amt <= -0.18': 0,\n",
    "    '-0.18 < amt <= 0.04': 0, \n",
    "    '0.04 < amt <= 0.26': 0,\n",
    "    '0.26 < amt <= 0.48': 0,\n",
    "    'amt > 0.48': 0\n",
    "    # ... many more binary features (All other features will be 0)\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 4: Apply Kernel Weighting**\n",
    "\n",
    "LIME uses this kernel weighting formula: \n",
    "\n",
    "$$\\pi_x(z) = \\exp\\left(-\\frac{D(x, z)^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $D(x, z)$ is the distance between original transaction $x$ and perturbed sample $z$ (binary distance will be count of features that differ)\n",
    "- $\\sigma$ controls how quickly weight decays with distance\n",
    "\n",
    "**Perturbed 1**: (`AK` changed, `CA` added)\n",
    "$$\\pi_x(z_1) = \\exp\\left(-\\frac{2^2}{1.5^2}\\right) = \\exp(-1.78) = 0.17$$\n",
    "\n",
    "**Perturbed 2**: (`shopping_pos` changed, `grocery_pos` added)\n",
    "$$\\pi_x(z_2) = \\exp\\left(-\\frac{2^2}{1.5^2}\\right) = \\exp(-1.78) = 0.17$$\n",
    "\n",
    "**Perturbed 3**: (`distance` bin, `age` bracket changed, new `age` bracket added)\n",
    "$$\\pi_x(z_3) = \\exp\\left(-\\frac{3^2}{1.5^2}\\right) = \\exp(-4) = 0.02$$\n",
    "\n",
    "\n",
    "##### **Step 5: Train Local Interpretable Model**\n",
    "\n",
    "LIME fits a weighted linear model to approximate the black box model locally:\n",
    "\n",
    "$$g(z) = \\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + \\cdots + \\beta_d z_d$$\n",
    "\n",
    "**Loss Function** (Minimize the weighted squared error):\n",
    "\n",
    "$$\\min_{\\beta_0, \\beta} \\sum_{i=1}^{n} \\pi_x(z_i) \\left( f(z_i) - \\big(\\beta_0 + \\beta^T z_i\\big) \\right)^2$$\n",
    "\n",
    "Where $f(z_i)$ is the black box prediction for perturbed transaction $z_i$.\n",
    "\n",
    "##### **Step 6: Interpret Feature Contributions**\n",
    "\n",
    "The coefficients (β) of the linear model show each feature's contribution:\n",
    "\n",
    "| **Feature**   | **Contribution** | **Interpretation**            |\n",
    "|---------------|:----------------:|-------------------------------|\n",
    "| **AK <= -0.06**        |      **+0.73**   | **Strongly indicates fraud**  |\n",
    "| **-0.60 < amt <= -0.46**       |      **+0.31**   | Moderately indicates fraud    |\n",
    "| **0.09 < dist <= 0.41**      |      **+0.26**   | Moderately indicates fraud    |\n",
    "| shopping_pos <= -0.33  |        +0.12     | Slightly indicates fraud      |\n",
    "| -0.79 < age <= -0.11           |        -0.08     | Slightly indicates legitimate |\n",
    "| F <= 0.91             |        -0.04     | Minimal impact                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = np.sqrt(X_train.shape[1]) * 0.75\n",
    "\n",
    "# LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train,\n",
    "    feature_names=features,\n",
    "    class_names=[0, 1],\n",
    "    categorical_features=categorical_features,\n",
    "    kernel_width=kernel_width\n",
    ")\n",
    "\n",
    "idx = misclassified_indices[1]\n",
    "\n",
    "print(f'Considering Index: `{idx}`')\n",
    "\n",
    "# Create a prediction function that returns probabilities for BOTH classes\n",
    "def model_predict_fn(x):\n",
    "    preds = model.predict(x)\n",
    "    two_column_preds = np.concatenate([1 - preds, preds], axis=1)\n",
    "    return two_column_preds\n",
    "    \n",
    "exp = explainer.explain_instance(\n",
    "    X_test[idx],\n",
    "    model_predict_fn,\n",
    "    num_features=10,\n",
    ")\n",
    "\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How to interpret LIME Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Guide:**\n",
    "* **Left side**: Prediction probability is shown \n",
    "* **Blue bars (left)**: Features contributing to \"legitimate\" prediction\n",
    "* **Orange bars (right)**: Features contributing to \"fraudulent\" prediction\n",
    "\n",
    "**Tabular View:**\n",
    "* Shows actual value for each feature\n",
    "* Highlights contribution to each outcome (legitimate or fraudulent)\n",
    "\n",
    "**Analysis Tips:**\n",
    "Pay attention to which transaction characteristics most strongly influence the fraud determination. For example:\n",
    "* Unusually large transaction amount\n",
    "* Atypical merchant category\n",
    "* Transaction occurring at unusual time\n",
    "\n",
    "These insights help financial analysts understand why the model flagged specific transactions, improving both accuracy of manual reviews and overall model transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors ⚓️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Precision Model-Agnostic Explanations (Anchors) [Paper, 2018](https://ojs.aaai.org/index.php/AAAI/article/view/11491)\n",
    "\n",
    "> *Anchors explain individual predictions with simple IF-THEN rules that capture key conditions behind a decision.*\n",
    "\n",
    "#### **Anchors Process in Fraud Detection:**\n",
    "1. **Generate Rule Candidates:** Propose simple rules that could explain a model's prediction.\n",
    "2. **Select Best Anchor:** Identify the rule that best explains the specific transaction.\n",
    "3. **Validate Precision:** Confirm the rule’s accuracy by testing it on similar cases.\n",
    "4. **Refine with Search:** Improve the rule using an efficient search algorithm.\n",
    "\n",
    "#### **Key Insights:**\n",
    "* **Anchors are IF-THEN rules** that provide clear, high-precision explanations.\n",
    "* They focus on **accuracy over coverage**: the rule is very reliable when it applies, even if it covers a small group.\n",
    "\n",
    "[Anchors Code Tutorial from original paper authors](https://github.com/marcotcr/anchor/blob/master/notebooks/Anchor%20on%20tabular%20data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Anchors explainer\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    class_names=['fraud', 'non-fraud'],\n",
    "    feature_names=features,\n",
    "    X_train.values,\n",
    "    categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample for explanation\n",
    "idx = 100\n",
    "\n",
    "# Print Prediction\n",
    "print('Prediction: ', explainer.class_names[model.predict(X_test.values[idx].reshape(1, -1))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction using Anchors\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict, threshold=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the prediction, precision, and coverage\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Interpret\n",
    "\n",
    "This rule means that if these conditions are met, the model’s fraud prediction is highly reliable. This explanation gives fraud analysts a clear rule they can understand and verify, rather than just a list of contributing factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeVbM_y2XCM8"
   },
   "source": [
    "## Single Feature Partial Dependence Plot\n",
    "\n",
    "[How to Build Shap Single Feature Partial Dependence Plot (PDP Plot)](https://www.youtube.com/watch?v=CgKyAlA-0wA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95709,
     "status": "aborted",
     "timestamp": 1742934046673,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "JPTXTAw9XCM8"
   },
   "outputs": [],
   "source": [
    "# Dependence plot for specific feature\n",
    "shap.dependence_plot(\"amt\", shap_values, X_test[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95712,
     "status": "aborted",
     "timestamp": 1742934046678,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "adUOWLEJXCM9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95720,
     "status": "aborted",
     "timestamp": 1742934046690,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "MOiCC9MlXCM9"
   },
   "outputs": [],
   "source": [
    "fraud_data = data_test[data_test['is_fraud'] == 1]\n",
    "non_fraud_data = data_test[data_test['is_fraud'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046696,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "dmMvfKQ6XCM9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of top n features\n",
    "for feature in top_n_features:\n",
    "    plt.hist(fraud_data[feature].astype(int), alpha=0.5, label='Fraud', bins=30)\n",
    "    plt.hist(non_fraud_data[feature].astype(int), alpha=0.5, label='Non-Fraud', bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046705,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "_T-ckdALXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'amt'\n",
    "mean_value = data_test[feature].mean()\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature] > mean_value]) * 100 / len(data_test[data_test[feature] > mean_value])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046707,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "BShKKF_EXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'shopping_net'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95728,
     "status": "aborted",
     "timestamp": 1742934046710,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "aGTPMOVPXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'grocery_pos'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046712,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "n57QfO-4XCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'gas_transport'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046714,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "KAdyBp_EXCM_"
   },
   "outputs": [],
   "source": [
    "feature = 'misc_net' # Miscellaneous online transactions\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046716,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "UA4ZUPBfXCM_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "state_columns = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'FL', 'GA', 'HI', 'IA', \n",
    "                'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', \n",
    "                'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', \n",
    "                'OR', 'PA', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "\n",
    "states = []\n",
    "fraud_rates = []\n",
    "transaction_counts = []\n",
    "fraud_counts = []\n",
    "\n",
    "# Calculate metrics for each state\n",
    "for state in state_columns:\n",
    "    state_transactions = data_test[data_test[state] == 1]\n",
    "    total = len(state_transactions)\n",
    "    \n",
    "    if total > 0:\n",
    "        fraud_count = state_transactions['is_fraud'].sum()\n",
    "        fraud_rate = fraud_count / total\n",
    "        \n",
    "        states.append(state)\n",
    "        fraud_rates.append(fraud_rate)\n",
    "        transaction_counts.append(total)\n",
    "        fraud_counts.append(fraud_count)\n",
    "\n",
    "state_fraud_df = pd.DataFrame({\n",
    "    'State': states,\n",
    "    'Fraud_Rate': fraud_rates,\n",
    "    'Transaction_Count': transaction_counts,\n",
    "    'Fraud_Count': fraud_counts\n",
    "})\n",
    "\n",
    "# Sort by fraud rate descending\n",
    "state_fraud_df = state_fraud_df.sort_values('Fraud_Rate', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.barplot(x='Fraud_Rate', y='State', data=state_fraud_df, palette='viridis')\n",
    "\n",
    "plt.title('Fraud Rate by State', fontsize=16)\n",
    "plt.xlabel('Fraud Rate', fontsize=12)\n",
    "plt.ylabel('State', fontsize=12)\n",
    "\n",
    "for i, v in enumerate(state_fraud_df['Fraud_Rate']):\n",
    "    ax.text(v + 0.005, i, f\"{v:.2%}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a second visualization - bubble chart with fraud rates and transaction volume\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "scatter = plt.scatter(state_fraud_df['Transaction_Count'], \n",
    "                     state_fraud_df['Fraud_Rate'], \n",
    "                     s=state_fraud_df['Fraud_Count']*5,\n",
    "                     alpha=0.7,\n",
    "                     c=state_fraud_df['Fraud_Rate'],\n",
    "                     cmap='Reds')\n",
    "\n",
    "for _, row in state_fraud_df.head(5).iterrows():\n",
    "    plt.annotate(row['State'], \n",
    "                (row['Transaction_Count'], row['Fraud_Rate']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold')\n",
    "\n",
    "plt.xscale('log')  # Use log scale for better visualization if counts vary widely\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Fraud Rate vs Transaction Volume by State', fontsize=16)\n",
    "plt.xlabel('Number of Transactions (log scale)', fontsize=12)\n",
    "plt.ylabel('Fraud Rate', fontsize=12)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Fraud Rate', rotation=270, labelpad=15)\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "legend1 = plt.legend(handles, labels, loc=\"upper left\", title=\"States\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My3D7T0iXCM_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Feature Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046718,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "ycm2Z80ZXCM_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def feature_ablation_study_global(model, X, y, from_idx, to_idx, selected_features, all_features):\n",
    "    global replacement_values, ablated_pred\n",
    "    base_pred = model.predict(X, verbose=0)\n",
    "    base_auc = roc_auc_score(y, base_pred)\n",
    "\n",
    "    # Precompute replacement values for each feature to avoid repeated computation.\n",
    "    replacement_values = {}\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        replacement_values[feature] = np.median(X[:, idx])\n",
    "\n",
    "    print(f\"Global (AUC) Feature Ablation Study from index [{from_idx}] to index [{to_idx}]:\")\n",
    "    test_set = X[from_idx:to_idx+1]\n",
    "    records = []\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        X_temp = test_set.copy()\n",
    "        X_temp[:, idx] = replacement_values[feature]\n",
    "\n",
    "        ablated_pred = model.predict(X_temp, verbose=0)\n",
    "        ablated_auc = roc_auc_score(y[from_idx:to_idx+1], ablated_pred)\n",
    "\n",
    "        impact = ((base_auc - ablated_auc) / base_auc) * 100\n",
    "\n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_auc': ablated_auc,\n",
    "            'impact_score': impact\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046720,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "4qWd05VDXCM_"
   },
   "outputs": [],
   "source": [
    "df_ablation_global = feature_ablation_study_global(model, X_test, y_test, from_idx, to_idx, top_n_features, features)\n",
    "df_ablation_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ2ugevLXCNA"
   },
   "source": [
    "### Local Interterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95728,
     "status": "aborted",
     "timestamp": 1742934046721,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "9FPI9wDIXCNA"
   },
   "outputs": [],
   "source": [
    "def feature_ablation_single_entry(model, data_entry, selected_features, all_features):\n",
    "    baseline_values = np.zeros_like(data_entry)\n",
    "\n",
    "    original_prediction = model.predict(data_entry.reshape(1, -1), verbose=0)\n",
    "\n",
    "    print(f\"Local Feature Ablation Study: \")\n",
    "    records = []\n",
    "    for i, feature in enumerate(all_features):\n",
    "        ablated_entry = data_entry.copy()\n",
    "        ablated_entry[i] = baseline_values[i]\n",
    "\n",
    "        ablated_pred = model.predict(ablated_entry.reshape(1, -1), verbose=0) # Compute prediction on the ablated entry\n",
    "\n",
    "        impact = original_prediction - ablated_pred # Calculate the drop (or change) in prediction\n",
    "\n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_auc': ablated_pred.squeeze(),\n",
    "            'impact_score': impact.squeeze()\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "\n",
    "    return df_results, original_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046722,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "12IJyedZXCNA"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "df_ablation_local, original_prob = feature_ablation_single_entry(model, X_test[idx], top_n_features, features)\n",
    "df_ablation_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046723,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "wJSufCzFXCNA"
   },
   "outputs": [],
   "source": [
    "# Plot feature contributions for a prediction\n",
    "shap.initjs()\n",
    "baseline = explainer.expected_value.numpy()\n",
    "\n",
    "shap.force_plot(baseline, shap_values[idx:idx+1], processed_data.loc[y_test.index].drop('is_fraud', axis=1).iloc[idx:idx+1], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046724,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "-rgfAovSXCNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046727,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "EagJ-FmRXCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046729,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "1m9NwVn1XCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046730,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "Hjmu2QJUXCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046731,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "_yOnzYEPXCND"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742934290024,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "cyFcK8J4-jta",
    "outputId": "f72e1336-c4cc-4d93-b53a-119c2dbccafc"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046736,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "fL6CdKzAbYtb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95723,
     "status": "aborted",
     "timestamp": 1742934046737,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "oJjgvocSXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95721,
     "status": "aborted",
     "timestamp": 1742934046738,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "jvL9JdiFXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046745,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "Wrm4cVrcXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046746,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "NyoEOV_zXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046751,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "xTQ6uswiXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8uIAv7vXCNE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95737,
     "status": "aborted",
     "timestamp": 1742934046761,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "91SSPOxpXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtDHMDDWXCNE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Not Active Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "def feature_ablation_study_local(model, X, y, index, selected_features, all_features):\n",
    "    global records, base_pred, base_diff, ablated_diff\n",
    "    base_pred = model.predict(X[index:index+1], verbose=0)\n",
    "    base_diff = y[index:index+1].to_numpy() - base_pred\n",
    "    \n",
    "    # Precompute replacement values for each feature to avoid repeated computation.\n",
    "    replacement_values = {}\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        replacement_values[feature] = np.median(X[:, idx])\n",
    "\n",
    "    print(f\"Local Feature Ablation Study of index [{index}]:\")\n",
    "    test_set = X[index:index+1]\n",
    "    records = []\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        X_temp = test_set.copy()\n",
    "        X_temp[:, idx] = replacement_values[feature]\n",
    "        \n",
    "        ablated_pred = model.predict(X_temp, verbose=0)\n",
    "        ablated_diff = y[index:index+1].to_numpy() - ablated_pred\n",
    "        \n",
    "        impact = np.mean(base_diff - ablated_diff / base_diff)\n",
    "        \n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_diff': np.mean(ablated_diff),\n",
    "            'impact_score': impact\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    \n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "df_ablation_local = feature_ablation_study_local(model, X_test, y_test, 2, top_n_features, features)\n",
    "df_ablation_local\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpxZSOGLXCMk"
   },
   "source": [
    "Source: https://github.com/siddharthapramanik771/CreditCardFraudDetectionML/blob/main/Credit_card_Fraud_Detection.ipynb\n",
    "\n",
    "Jira: https://laiminhthong1.atlassian.net/jira/core/projects/CCTFDUX/board\n",
    "\n",
    "Github Repos: https://github.com/ThongLai/Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI\n",
    "\n",
    "Main Dataset: https://www.kaggle.com/datasets/kartik2112/fraud-detection/data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv_xai_fraud_detection",
   "language": "python",
   "name": ".venv_xai_fraud_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
