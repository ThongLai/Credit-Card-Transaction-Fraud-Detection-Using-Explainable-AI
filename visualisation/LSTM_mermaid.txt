---
config:
  layout: elk
---
flowchart LR
 subgraph inputs["Input Layer (15 features)"]
        i1(("•"))
        i2(("•"))
        i3(("•"))
        i4(("•"))
        i5(("•"))
        i6(("•"))
        i15[("•<br>↓<br>•")]
  end
 subgraph reshape["Reshape λ"]
        rs[("(batch,1,15)")]
  end
 subgraph lstm1_cell1["Cell 1"]
    direction TB
        f1(("f"))
        i1g(("i"))
        c1(("c"))
        o1(("o"))
  end
 subgraph lstm1_cell50["Cell 50"]
    direction TB
        f1_50(("f"))
        i1_50(("i"))
        c1_50(("c"))
        o1_50(("o"))
  end
 subgraph lstm1["LSTM Layer 1 (50 units)"]
        lstm1_cell1
        lstm1_dots["..."]
        lstm1_cell50
  end
 subgraph lstm2_cell1["Cell 1"]
    direction TB
        f2_1(("f"))
        i2_1(("i"))
        c2_1(("c"))
        o2_1(("o"))
  end
 subgraph lstm2_cell50["Cell 50"]
    direction TB
        f2_50(("f"))
        i2_50(("i"))
        c2_50(("c"))
        o2_50(("o"))
  end
 subgraph lstm2["LSTM Layer 2 (50 units)"]
        lstm2_cell1
        lstm2_dots["..."]
        lstm2_cell50
  end
 subgraph attMech["Attention Mechanism"]
    direction TB
        att_w[("W<sub>50×1</sub>")]
        att_b[("b<sub>1×1</sub>")]
        att_tanh{{"tanh()"}}
        att_soft{{"softmax"}}
        att_weight[("α<sub>weights</sub>")]
        att_mul{{"×"}}
        att_sum{{"Σ"}}
  end
 subgraph attention["Attention Layer (50 output units)"]
        attMech
        att1(("•"))
        att2(("•"))
        att3(("•"))
        att25[("•<br>↓<br>•")]
        att48(("•"))
        att49(("•"))
        att50(("•"))
  end
 subgraph dense["Dense Layer (sigmoid)"]
        d1(("σ<br>output"))
  end
    f1 -- forget --> c1
    i1g -- input --> c1
    c1 -- output --> o1
    f1_50 -- forget --> c1_50
    i1_50 -- input --> c1_50
    c1_50 -- output --> o1_50
    f2_1 -- forget --> c2_1
    i2_1 -- input --> c2_1
    c2_1 -- output --> o2_1
    f2_50 -- forget --> c2_50
    i2_50 -- input --> c2_50
    c2_50 -- output --> o2_50
    att_w -- compute --> att_tanh
    att_b -- compute --> att_tanh
    att_tanh -- normalize --> att_soft
    att_soft -- attention<br>scores --> att_weight
    att_weight -- apply --> att_mul
    att_mul -- context<br>vector --> att_sum
    inputs -- features --> reshape
    reshape -- sequenced<br>input --> lstm1
    lstm1_cell1 -- hidden<br>states --> lstm1_dots
    lstm1_cell50 -- hidden<br>states --> lstm1_dots
    lstm1 -- temporal<br>features --> lstm2
    lstm2_cell1 -- hidden<br>states --> lstm2_dots
    lstm2_cell50 -- hidden<br>states --> lstm2_dots
    lstm2 -- sequence<br>representation --> attMech
    attMech -- focused<br>features --> att1 & att2 & att3 & att25 & att48 & att49 & att50
    att1 -- weighted<br>inputs --> d1
    att2 -- weighted<br>inputs --> d1
    att3 -- weighted<br>inputs --> d1
    att25 -- weighted<br>inputs --> d1
    att48 -- weighted<br>inputs --> d1
    att49 -- weighted<br>inputs --> d1
    att50 -- weighted<br>inputs --> d1
    i1 -. input<br>connection .- lstm1_cell1
    i3 -. input<br>connection .- lstm1_cell1
    i5 -. input<br>connection .- lstm1_cell50
    i15 -. input<br>connection .- lstm1_cell50
    o1 -. recurrent<br>connection .- lstm2_cell1
    o1_50 -. recurrent<br>connection .- lstm2_cell50
    o2_1 -. attention<br>input .- att_w
    o2_50 -. attention<br>input .- att_w
    lstm2 -. sequence<br>data .- att_mul
     i1:::input
     i2:::input
     i3:::input
     i4:::input
     i5:::input
     i6:::input
     i15:::input
     rs:::reshape
     f1:::lstmGate
     i1g:::lstmGate
     c1:::lstmGate
     o1:::lstmGate
     lstm1_dots:::dots
     f1_50:::lstmGate
     i1_50:::lstmGate
     c1_50:::lstmGate
     o1_50:::lstmGate
     f2_1:::lstmGate
     i2_1:::lstmGate
     c2_1:::lstmGate
     o2_1:::lstmGate
     lstm2_dots:::dots
     f2_50:::lstmGate
     i2_50:::lstmGate
     c2_50:::lstmGate
     o2_50:::lstmGate
     att_w:::attWeight
     att_b:::attWeight
     att_tanh:::attWeight
     att_soft:::attWeight
     att_weight:::attWeight
     att_mul:::attWeight
     att_sum:::attWeight
     att1:::attNeuron
     att2:::attNeuron
     att3:::attNeuron
     att25:::attNeuron
     att48:::attNeuron
     att49:::attNeuron
     att50:::attNeuron
     d1:::denseNeuron
    classDef input fill:#0099ff,stroke:#0066cc,stroke-width:3px,color:white,font-weight:bold,font-size:16px
    classDef reshape fill:#9c27b0,stroke:#7b1fa2,stroke-width:3px,color:white,font-weight:bold,font-size:16px
    classDef lstmCell fill:#ff9800,stroke:#e65100,stroke-width:3px,color:white,font-weight:bold,font-size:16px
    classDef lstmGate fill:#ffb74d,stroke:#ef6c00,stroke-width:2px,color:black,font-weight:bold,font-size:14px
    classDef attWeight fill:#4caf50,stroke:#2e7d32,stroke-width:2px,color:white,font-weight:bold,font-size:14px
    classDef attNeuron fill:#8bc34a,stroke:#558b2f,stroke-width:3px,color:white,font-weight:bold,font-size:16px
    classDef denseNeuron fill:#673ab7,stroke:#4527a0,stroke-width:3px,color:white,font-weight:bold,font-size:18px
    classDef dots fill:#f5f5f5,stroke:#9e9e9e,stroke-width:2px,color:#424242,font-weight:bold,font-size:20px
    style inputs fill:#e3f2fd,stroke:#1565c0,stroke-width:4px,color:#0d47a1,font-weight:bold,font-size:18px
    style reshape fill:#f3e5f5,stroke:#8e24aa,stroke-width:4px,color:#4a148c,font-weight:bold,font-size:18px
    style lstm1 fill:#fff3e0,stroke:#e65100,stroke-width:4px,color:#bf360c,font-weight:bold,font-size:18px
    style lstm1_cell1 fill:#ffe0b2,stroke:#fb8c00,stroke-width:3px,color:#e65100,font-weight:bold
    style lstm1_cell50 fill:#ffe0b2,stroke:#fb8c00,stroke-width:3px,color:#e65100,font-weight:bold
    style lstm2 fill:#ffebee,stroke:#c62828,stroke-width:4px,color:#b71c1c,font-weight:bold,font-size:18px
    style lstm2_cell1 fill:#ffcdd2,stroke:#ef5350,stroke-width:3px,color:#c62828,font-weight:bold
    style lstm2_cell50 fill:#ffcdd2,stroke:#ef5350,stroke-width:3px,color:#c62828,font-weight:bold
    style attention fill:#e8f5e9,stroke:#2e7d32,stroke-width:4px,color:#1b5e20,font-weight:bold,font-size:18px
    style attMech fill:#c8e6c9,stroke:#43a047,stroke-width:3px,color:#2e7d32,font-weight:bold
    style dense fill:#e1f5fe,stroke:#0288d1,stroke-width:4px,color:#01579b,font-weight:bold,font-size:18px
    classDef subgraphLabel font-size:18px,stroke-width:2px,color:#00000
    class dense,attention,attMech,lstm2,lstm2_cell50,lstm2_cell1,lstm1,lstm1_cell50,lstm1_cell1,reshape,inputs subgraphLabel