{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XAI** Methods Implementation: **Credit Card Fraud Detection** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies cutting-edge Explainable AI (XAI) techniques to analyze credit card fraud detection models. By implementing SHAP, LIME, and Anchors, we transform \"black box\" neural networks into transparent, interpretable systems that provide clear explanations for fraud predictions. \n",
    "\n",
    "All necessary models, dataset, libraries are imported and should be ready to use, with pre-trained models available in the `architectures/` folder.\n",
    "\n",
    "- **Github Repos:** [Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI](https://github.com/ThongLai/Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI)\n",
    "\n",
    "- **Run live notebook:** [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ThongLai/Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI/main?urlpath=%2Fdoc%2Ftree%2FXAI_methods.ipynb)\n",
    "\n",
    "- **Models: [architectures](https://github.com/ThongLai/Credit-Card-Transaction-Fraud-Detection-Using-Explainable-AI/tree/main/architectures)**<a name=\"models\" id=\"models\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Thong Minh Lai \n",
    "\n",
    "**Last Updated:** 04/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What This Notebook Contains:**\n",
    "\n",
    "- **Pre-trained Black Box Models**: Collection of pre-trained fraud detection models\n",
    "- **Global Interpretability**: Understanding overall model behavior and feature importance\n",
    "- **Local Explanations**: Detailed analysis of individual transaction predictions\n",
    "- **Visual Insights**: Interactive visualizations showing why specific transactions are flagged\n",
    "- **Comparative Analysis**: Multiple XAI techniques applied to the same predictions\n",
    "\n",
    "**Key XAI Techniques Implemented**\n",
    "\n",
    "1. **SHAP (SHapley Additive exPlanations)** üé≤\n",
    "   - Measures each feature's contribution to predictions using game theory\n",
    "   - Provides both global importance and transaction-specific explanations\n",
    "\n",
    "2. **LIME (Local Interpretable Model-agnostic Explanations)** üçã\n",
    "   - Creates simple models that approximate complex models locally\n",
    "   - Shows which features influenced specific fraud predictions\n",
    "\n",
    "3. **Anchors** ‚öì\n",
    "   - Generates clear IF-THEN rules that explain model decisions\n",
    "   - Focuses on high-precision, easy-to-understand explanations\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "Understanding fraud detection models through explainable AI is essential in today's financial landscape. Financial institutions face increasing regulatory pressure for transparency in automated decision-making [[1](https://ijsra.net/content/explainable-ai-financial-technologies-balancing-innovation-regulatory-compliance)], while simultaneously needing to improve model performance to combat sophisticated fraud techniques [[2](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4980350#:~:text=This%20study%20examines%20the,consisting%20of%20284%2C807%20transactions)]. Explainable fraud detection creates a crucial bridge between complex AI systems and human oversight, enabling compliance officers to validate regulatory adherence and fraud analysts to verify and refine AI-flagged transactions with their domain expertise [[3](https://www.researchgate.net/publication/226538138_Trust_and_Stakeholder_Theory_Trustworthiness_in_the_Organisation-Stakeholder_Relationship#:~:text=Trust%20is%20a%20fundamental,stakeholders%20within%20the%20organization‚Äìstakeholder)]. This transparency builds stakeholder trust by demonstrating that fraud decisions aren't emerging from an algorithmic \"black box\" but are based on identifiable, reasonable patterns that can be communicated to customers, auditors, and management. As financial fraud becomes more sophisticated, this human-AI partnership represents the most effective defense, combining the pattern-recognition capabilities of neural networks with human judgment and regulatory compliance requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Setting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'architectures/'\n",
    "DATASET_PATH = 'dataset/'\n",
    "MAKE_PREDICTIONS = False # Make predictions from loaded models, leave as `Fasle` to load predictions from `predictions.csv` instead\n",
    "RANDOM_SEED = 42 # Set to `None` for the generator uses the current system time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on `Binder`, then it is no need to set up the packages again\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# ---OR---\n",
    "\n",
    "# %pip install tensorflow==2.10.1 numpy==1.26.4 pandas scikit-learn imblearn matplotlib seaborn requests shap lime anchor-exp dice_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# XAI\n",
    "import shap\n",
    "from lime import lime_tabular\n",
    "from anchor import anchor_tabular\n",
    "import dice_ml\n",
    "\n",
    "import os\n",
    "import time\n",
    "import utils\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import test dataset and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.download_dataset_from_kaggle('fraudTrain.csv')\n",
    "utils.download_dataset_from_kaggle('fraudTest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(os.path.join(DATASET_PATH, 'fraudTrain.csv'), index_col=0)\n",
    "data_test = pd.read_csv(os.path.join(DATASET_PATH, 'fraudTest.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = utils.feature_engineering(data_train)\n",
    "X_train, y_train, data_train, transformations_train = utils.pre_processing(data_train)\n",
    "\n",
    "data_test = utils.feature_engineering(data_test)\n",
    "X_test, y_test, data_test, transformations_test = utils.pre_processing(data_test, isTestSet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-trained models and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = utils.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_PREDICTIONS:\n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        y_predict = model.predict(X_test)\n",
    "        predictions[model_name] = y_predict.flatten()\n",
    "\n",
    "        # Save predictions\n",
    "        utils.save_predictions(model_name, y_predict)\n",
    "else:\n",
    "    predictions = pd.read_csv(os.path.join(DATASET_PATH, 'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary\n",
    "model_name = predictions.keys()[0]\n",
    "y_predict = predictions[model_name]\n",
    "y_predict_binary = np.round(y_predict).astype(int).squeeze()\n",
    "model = models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Dataset Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field Name | Description |\n",
    "|------------|-------------|\n",
    "| **trans_date_trans_time** | Date and time when transaction occurred |\n",
    "| **cc_num** | Credit card number of customer |\n",
    "| **merchant** | Name of merchant where transaction occurred |\n",
    "| **category** | Category of merchant (e.g., retail, food, etc.) |\n",
    "| **amt** | Amount of transaction |\n",
    "| **first** | First name of credit card holder |\n",
    "| **last** | Last name of credit card holder |\n",
    "| **gender** | Gender of credit card holder |\n",
    "| **street** | Street address of credit card holder |\n",
    "| **city** | City of credit card holder |\n",
    "| **state** | State of credit card holder |\n",
    "| **zip** | ZIP code of credit card holder |\n",
    "| **lat** | Latitude location of credit card holder |\n",
    "| **long** | Longitude location of credit card holder |\n",
    "| **city_pop** | Population of credit card holder's city |\n",
    "| **job** | Occupation of credit card holder |\n",
    "| **dob** | Date of birth of credit card holder |\n",
    "| **trans_num** | Transaction number |\n",
    "| **unix_time** | UNIX timestamp of transaction |\n",
    "| **merch_lat** | Latitude location of merchant |\n",
    "| **merch_long** | Longitude location of merchant |\n",
    "| **is_fraud** | Target class indicating whether transaction is fraudulent (1) or legitimate (0) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKeyM24MXCM5"
   },
   "source": [
    "## XAI Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOYJJGKJXCM6"
   },
   "source": [
    "### SHAP üé≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBPavCPaXCM6"
   },
   "source": [
    "#### Get SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DeepExplainer`(specifcifically for neural networks)\n",
    "\n",
    "**For `DeepExplainer`, we need to create a `background` dataset**: This is because deep neural networks are complex and non-linear, so they require reference points (background samples) to understand how the model normally behaves and accurately calculate feature importance.\n",
    "\n",
    "Passing the entire training dataset as data will give very accurate expected values, but be unreasonably expensive. The variance of the expectation estimates scale by roughly `1/sqrt(N)` for `N` background data samples.\n",
    "\n",
    "So 100 samples will give a good estimate, and 1000 samples a very good estimate of the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features\n",
    "features = data_test.columns.drop('is_fraud').tolist()\n",
    "\n",
    "# Collect all categorical features\n",
    "categorical_features = list(data_test.select_dtypes(include=['bool', 'category', 'object']).columns)\n",
    "\n",
    "# Collect all misclassified entries (For later explaination on why the model predicted them incorrectly)\n",
    "misclassified_indices = np.where(y_test != y_predict_binary)[0]\n",
    "print(f\"Found {len(misclassified_indices)} misclassified instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742934211210,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "pGf_Bvq9XCM6"
   },
   "outputs": [],
   "source": [
    "def SHAP(model, X_train, X_test, from_idx, to_idx, background_size=100):\n",
    "    X_train = np.expand_dims(X_train, axis=-1) if X_train.shape[-1] != 1 else X_train\n",
    "    X_test = np.expand_dims(X_test, axis=-1) if X_test.shape[-1] != 1 else X_test\n",
    "\n",
    "    background = X_train[np.random.choice(len(X_train), background_size, replace=False)]\n",
    "\n",
    "    explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test[from_idx:to_idx+1]) # Deep learning models expect 2D input arrays (samples √ó features), X_test[idx] only returns a 1D array (shape: (n_features,)\n",
    "\n",
    "    shap_values = shap_values.squeeze()[np.newaxis, ...] if shap_values.shape[0] == 1 else shap_values.squeeze()\n",
    "\n",
    "    return explainer, shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9_o30OgXCM6"
   },
   "source": [
    "#### Global Interpretability (whole test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "error",
     "timestamp": 1742934213171,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "lGaL7LjtXCM7",
    "outputId": "0edda4ac-8f9d-4cda-dbfa-ae894882047d"
   },
   "outputs": [],
   "source": [
    "# Call the function to obtain SHAP values.\n",
    "from_idx = misclassified_indices[0]\n",
    "to_idx = misclassified_indices[0]\n",
    "# to_idx = len(X_test)-1\n",
    "\n",
    "explainer, shap_values = SHAP(model, X_train, X_test, from_idx, to_idx, background_size=100)\n",
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95704,
     "status": "aborted",
     "timestamp": 1742934046661,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "rFIzKUE7XCM7"
   },
   "outputs": [],
   "source": [
    "def get_top_n_features(shap_values, features, n=10):\n",
    "    mean_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    df_shap = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'mean_abs_shap': np.squeeze(mean_shap_values)\n",
    "    }).set_index('feature')\n",
    "\n",
    "    df_shap = df_shap.reindex(df_shap['mean_abs_shap'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "    # Get top n features\n",
    "    n = 10\n",
    "    top_n_features = list(df_shap.head(n).index)\n",
    "\n",
    "    display(df_shap.head(n))\n",
    "\n",
    "    return top_n_features\n",
    "\n",
    "top_n_features = get_top_n_features(shap_values, features, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a496_dzwXCM7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Visualization\n",
    "[SHAP Plots Explained](https://www.youtube.com/playlist?list=PLpoCVQU4m6j9HDOzRBL4nX4eol9DrZ3Kd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFdpTO9CXCM7"
   },
   "source": [
    "#### Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95709,
     "status": "aborted",
     "timestamp": 1742934046668,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "VeMFhKzCXCM8"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neoV_RfTXCM8"
   },
   "source": [
    "#### Force Plot\n",
    "\n",
    "[How to use Shapley Additive Explanations for Black Box Machine Learning Algorithms](https://www.youtube.com/watch?v=7wnG6Wnm2uU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95708,
     "status": "aborted",
     "timestamp": 1742934046670,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "FG_GjhTWXCM8"
   },
   "outputs": [],
   "source": [
    "# Plot feature contributions for a prediction\n",
    "shap.initjs()\n",
    "baseline = explainer.expected_value.numpy()\n",
    "\n",
    "shap.force_plot(baseline, shap_values, data_test.loc[y_test.index].drop('is_fraud', axis=1).iloc[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME üçã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Local Interpretable Model-Agnostic Explanations (LIME) [Paper, 2016](https://arxiv.org/abs/1602.04938)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Interpretable models that are used to explain individual predictions of black box machine learning models (for credit card fraud detection in this project)*\n",
    "\n",
    "##### LIME Process in Fraud Detection:\n",
    "1. **Select:** Choose a transaction (e.g., a potential fraud case).\n",
    "2. **Perturb:** Create variations by slightly altering its features.\n",
    "3. **Generate:** Build a dataset of these perturbed transactions with their fraud predictions.\n",
    "4. **Train:** Fit an easy-to-interpret model on this new dataset, giving more weight to samples similar to the original transaction.\n",
    "5. **Interpret:** Use the simple model to show which features drove the fraud prediction.\n",
    "\n",
    "##### Technical Implementation:\n",
    "* **Numerical Features:** Compute statistics (mean, std) and bin values (e.g., into quartiles).\n",
    "* **Categorical Features:** Calculate the frequency of each category.\n",
    "\n",
    "##### Key Parameter:\n",
    "* **Kernel Width:**  \n",
    "  - **Small width:** Only very similar transactions influence the explanation (high precision).  \n",
    "  - **Large width:** More diverse transactions are included (wider coverage).\n",
    "\n",
    "LIME helps make fraud detection more transparent by clarifying why a specific transaction was flagged as suspicious.\n",
    "\n",
    "[LIME Code Tutorial from original paper authors](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Original Transaction Instance**\n",
    "\n",
    "```python\n",
    "transaction = {\n",
    "    'amt': 1850.75,        # Transaction amount\n",
    "    'age': 27,             # Customer age\n",
    "    'dist': 792.3,         # Distance from home location\n",
    "    'F': 1,                # Female gender\n",
    "    'M': 0,                # Male gender\n",
    "    '20 to 30': 1,         # Age bracket\n",
    "    'AK': 1,               # State (Alaska)\n",
    "    'shopping_pos': 1,     # Transaction category\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 1: Generate Perturbations**\n",
    "\n",
    "Creating slightly modified versions (perturbations) of the original transaction by adding small random noise based on the feature's statistics (mean and standard deviation):\n",
    "\n",
    "```python\n",
    "perturbed_transactions = [\n",
    "    {    # Perturbed 1 (`AK` changed, `CA` added)\n",
    "        'amt': 1750.25, 'age': 27, 'dist': 792.3, \n",
    "        'F': 1, 'M': 0, '20 to 30': 1, \n",
    "        'AK': 0, 'CA': 1, 'shopping_pos': 1\n",
    "    },\n",
    "    {    # Perturbed 2 (`shopping_pos` `changed`, `grocery_pos` added)\n",
    "        'amt': 1850.75, 'age': 27, 'dist': 792.3,\n",
    "        'F': 1, 'M': 0, '20 to 30': 1, \n",
    "        'AK': 1, 'shopping_pos': 0, 'grocery_pos': 1\n",
    "    },\n",
    "    {    # Perturbed 3 (`distance`, `age` changed)\n",
    "        'amt': 1850.75, 'age': 42, 'dist': 156.7,\n",
    "        'F': 1, 'M': 0, '20 to 30': 0, '40 to 50': 1, \n",
    "        'AK': 1, 'shopping_pos': 1\n",
    "    },\n",
    "    # ... many more variations\n",
    "]\n",
    "\n",
    "# Get predictions from the black box model\n",
    "predictions = [\n",
    "    0.35,  # Transaction 1 - lower probability of fraud\n",
    "    0.42,  # Transaction 2\n",
    "    0.28,  # Transaction 3\n",
    "    # ... and so on\n",
    "]\n",
    "```\n",
    "\n",
    "##### **Step 2: Analyze Feature Distributions**\n",
    "LIME analyzes the distribution of values in the perturbed samples:\n",
    "```python\n",
    "# Feature statistics for discretization\n",
    "amt_stats = {\n",
    "    'mean': 1523.45,\n",
    "    'std': 342.87,\n",
    "    'thresholds': [-0.60, -0.46, -0.32, -0.18, 0.04, 0.26, 0.48]  # normalized\n",
    "}\n",
    "\n",
    "dist_stats = {\n",
    "    'mean': 457.23,\n",
    "    'std': 389.52,\n",
    "    'thresholds': [-0.82, -0.51, -0.20, 0.09, 0.41, 0.75, 1.08]  # normalized\n",
    "}\n",
    "\n",
    "# ...\n",
    "\n",
    "For binary categorical features thresholds, LIME typically uses: The percent point function (ppf) of the normal distribution + A small adjustment factor (typically around 0.4 to 0.6) + A small shift constant (often between -0.1 and 0.1)\n",
    "\n",
    "# Categorical feature analysis\n",
    "AK_stats = {\n",
    "    'frequency': 0.03,\n",
    "    'ppf_calculation': -1.88,  # (ppf(0.03) ‚âà -1.88) Inverse of standard normal CDF at 0.03 \n",
    "    'threshold': -0.06,  # -1.88 * 0.03 - 0.00 ‚âà -0.06 (Low adjustment factor (0.03) used due to feature rarity; no shift needed)\n",
    "}\n",
    "\n",
    "shopping_pos_stats = {\n",
    "    'frequency': 0.15,\n",
    "    'ppf_calculation': -1.04, # (ppf(0.15) ‚âà -1.04) Inverse of standard normal CDF at 0.15\n",
    "    'threshold': -0.33,  # -1.04 * 0.3 + 0.0 ‚âà -0.312 (Medium adjustment factor (0.3); no shift applied as base calculation was close to desired scale)\n",
    "}\n",
    "\n",
    "grocery_pos_stats = {\n",
    "    'frequency': 0.22,\n",
    "    'ppf_calculation': -0.77, # (ppf(0.22) ‚âà -0.77) Inverse of standard normal CDF at 0.22\n",
    "    'threshold': -0.43,  # -0.77 * 0.45 - 0.08 ‚âà -0.427 (Medium-high adjustment (0.45) with small negative shift to maintain consistent relationship with `shopping_pos`)\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 3: Discretize Continuous Features**\n",
    "LIME converts continuous features into binary features using thresholds:\n",
    "```python\n",
    "# Original transaction (normalized)\n",
    "normalized_transaction = {\n",
    "    'amt': -0.51,  # (1850.75 - mean) / std\n",
    "    'dist': 0.39,  # (792.3 - mean) / std\n",
    "    # ...other features\n",
    "}\n",
    "\n",
    "# Binary features after discretization\n",
    "binary_features = {\n",
    "    '-0.60 < amt <= -0.46': 1,  # True\n",
    "    '0.09 < dist <= 0.41': 1,   # True\n",
    "    'AK <= -0.06': 1,                    # True\n",
    "    'F <= 0.91': 1,                     # True\n",
    "    'shopping_pos <= -0.33': 1,          # True\n",
    "    # ...\n",
    "    'amt <= -0.60': 0, \n",
    "    '-0.60 < amt <= -0.46': 0, \n",
    "    '-0.46 < amt <= -0.32': 0,\n",
    "    '-0.32 < amt <= -0.18': 0,\n",
    "    '-0.18 < amt <= 0.04': 0, \n",
    "    '0.04 < amt <= 0.26': 0,\n",
    "    '0.26 < amt <= 0.48': 0,\n",
    "    'amt > 0.48': 0\n",
    "    # ... many more binary features (All other features will be 0)\n",
    "}\n",
    "```\n",
    "\n",
    "##### **Step 4: Apply Kernel Weighting**\n",
    "\n",
    "LIME uses this kernel weighting formula: \n",
    "\n",
    "$$\\pi_x(z) = \\exp\\left(-\\frac{D(x, z)^2}{\\sigma^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $D(x, z)$ is the distance between original transaction $x$ and perturbed sample $z$ (binary distance will be count of features that differ)\n",
    "- $\\sigma$ controls how quickly weight decays with distance\n",
    "\n",
    "**Perturbed 1**: (`AK` changed, `CA` added)\n",
    "$$\\pi_x(z_1) = \\exp\\left(-\\frac{2^2}{1.5^2}\\right) = \\exp(-1.78) = 0.17$$\n",
    "\n",
    "**Perturbed 2**: (`shopping_pos` changed, `grocery_pos` added)\n",
    "$$\\pi_x(z_2) = \\exp\\left(-\\frac{2^2}{1.5^2}\\right) = \\exp(-1.78) = 0.17$$\n",
    "\n",
    "**Perturbed 3**: (`distance` bin, `age` bracket changed, new `age` bracket added)\n",
    "$$\\pi_x(z_3) = \\exp\\left(-\\frac{3^2}{1.5^2}\\right) = \\exp(-4) = 0.02$$\n",
    "\n",
    "\n",
    "##### **Step 5: Train Local Interpretable Model**\n",
    "\n",
    "LIME fits a weighted linear model to approximate the black box model locally:\n",
    "\n",
    "$$g(z) = \\beta_0 + \\beta_1 z_1 + \\beta_2 z_2 + \\cdots + \\beta_d z_d$$\n",
    "\n",
    "**Loss Function** (Minimize the weighted squared error):\n",
    "\n",
    "$$\\min_{\\beta_0, \\beta} \\sum_{i=1}^{n} \\pi_x(z_i) \\left( f(z_i) - \\big(\\beta_0 + \\beta^T z_i\\big) \\right)^2$$\n",
    "\n",
    "Where $f(z_i)$ is the black box prediction for perturbed transaction $z_i$.\n",
    "\n",
    "##### **Step 6: Interpret Feature Contributions**\n",
    "\n",
    "The coefficients (Œ≤) of the linear model show each feature's contribution:\n",
    "\n",
    "| **Feature**   | **Contribution** | **Interpretation**            |\n",
    "|---------------|:----------------:|-------------------------------|\n",
    "| **AK <= -0.06**        |      **+0.73**   | **Strongly indicates fraud**  |\n",
    "| **-0.60 < amt <= -0.46**       |      **+0.31**   | Moderately indicates fraud    |\n",
    "| **0.09 < dist <= 0.41**      |      **+0.26**   | Moderately indicates fraud    |\n",
    "| shopping_pos <= -0.33  |        +0.12     | Slightly indicates fraud      |\n",
    "| -0.79 < age <= -0.11           |        -0.08     | Slightly indicates legitimate |\n",
    "| F <= 0.91             |        -0.04     | Minimal impact                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_names = {features.index(col): transformations_test[col].classes_.tolist() for col in categorical_features}\n",
    "categorical_idx = list(categorical_names.keys())\n",
    "kernel_width = np.sqrt(len(features)) * 0.75\n",
    "\n",
    "# LIME explainer\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=transformations_train['scaler'].inverse_transform(X_train),\n",
    "    class_names=['non-fraud', 'fraud'],\n",
    "    feature_names=features,\n",
    "    categorical_features=categorical_idx,\n",
    "    categorical_names=categorical_names,\n",
    "    kernel_width=kernel_width\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to explain data entry at `idx`\n",
    "idx = misclassified_indices[100]\n",
    "data_entry = transformations_test['scaler'].inverse_transform(X_test[[idx]]).squeeze()\n",
    "\n",
    "print(f'Considering Index: `{idx}`')\n",
    "\n",
    "# Create a prediction function that returns probabilities for BOTH classes\n",
    "def model_predict_fn(x):\n",
    "    x = transformations_test['scaler'].transform(x)\n",
    "    preds = model.predict(x)\n",
    "    two_column_preds = np.concatenate([1 - preds, preds], axis=1)\n",
    "    return two_column_preds\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_entry,\n",
    "    model_predict_fn,\n",
    "    num_features=len(features)\n",
    ")\n",
    "\n",
    "exp.show_in_notebook()\n",
    "\n",
    "data_test.iloc[[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **How to interpret LIME Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Guide:**\n",
    "* **Left side**: Prediction probability is shown \n",
    "* **Blue bars (left)**: Features contributing to \"legitimate\" prediction\n",
    "* **Orange bars (right)**: Features contributing to \"fraudulent\" prediction\n",
    "\n",
    "**Tabular View:**\n",
    "* Shows actual value for each feature\n",
    "* Highlights contribution to each outcome (legitimate or fraudulent)\n",
    "\n",
    "**Analysis Tips:**\n",
    "Pay attention to which transaction characteristics most strongly influence the fraud determination. For example:\n",
    "* Unusually large transaction amount\n",
    "* Atypical merchant category\n",
    "* Transaction occurring at unusual time\n",
    "\n",
    "These insights help financial analysts understand why the model flagged specific transactions, improving both accuracy of manual reviews and overall model transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors ‚öìÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Precision Model-Agnostic Explanations (Anchors) [Paper, 2018](https://ojs.aaai.org/index.php/AAAI/article/view/11491)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Anchors explain individual predictions with simple IF-THEN rules that capture key conditions behind a decision.*\n",
    "\n",
    "##### **Anchors Process in Fraud Detection:**\n",
    "1. **Generate Rule Candidates:** Propose simple rules that could explain a model's prediction.\n",
    "2. **Select Best Anchor:** Identify the rule that best explains the specific transaction.\n",
    "3. **Validate Precision:** Confirm the rule‚Äôs accuracy by testing it on similar cases.\n",
    "4. **Refine with Search:** Improve the rule using an efficient search algorithm.\n",
    "\n",
    "##### **Key Insights:**\n",
    "* **Anchors are IF-THEN rules** that provide clear, high-precision explanations.\n",
    "* They focus on **accuracy over coverage**: the rule is very reliable when it applies, even if it covers a small group.\n",
    "\n",
    "[Anchors Code Tutorial from original paper authors](https://github.com/marcotcr/anchor/blob/master/notebooks/Anchor%20on%20tabular%20data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Anchors explainer with the sample\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    train_data=transformations_train['scaler'].inverse_transform(X_train), \n",
    "    class_names=['non-fraud', 'fraud'],\n",
    "    feature_names=features,\n",
    "    categorical_names=categorical_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to explain data entry at `idx`\n",
    "idx = misclassified_indices[0]\n",
    "data_entry = transformations_test['scaler'].inverse_transform(X_test[[idx]]).squeeze()\n",
    "\n",
    "print(f'Considering Index: `{idx}`')\n",
    "\n",
    "# Explain the prediction using Anchors\n",
    "def model_predict_fn(x):\n",
    "    x = transformations_test['scaler'].transform(x)\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    preds_binary = np.round(preds).astype(int).flatten()\n",
    "    return preds_binary\n",
    "\n",
    "# Print the anchor explanation\n",
    "def print_anchor_explanation(exp, instance_prediction, data_entry, feature_names):\n",
    "    \"\"\"Anchor explanation printer\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ANCHOR EXPLANATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print the anchor rules\n",
    "    if exp.names():\n",
    "        print(\"\\nIF THESE CONDITIONS ARE MET:\")\n",
    "        for i, condition in enumerate(exp.names(), 1):\n",
    "            print(f\"   {i}. {condition}\")\n",
    "        print(f\"\\nTHEN: Prediction is `{explainer.class_names[instance_prediction]}`\")\n",
    "    else:\n",
    "        print(f\"No specific rules found. Prediction is `{explainer.class_names[instance_prediction]}`\")\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nFEATURE VALUES FOR THIS INSTANCE:\")\n",
    "    print(f\"  ‚Ä¢ Precision: {exp.precision():.2f} ‚Üí If these conditions are met, the prediction is the same {exp.precision()*100:.1f}% of the time\")\n",
    "    print(f\"  ‚Ä¢ Coverage: {exp.coverage():.2f} ‚Üí These conditions apply to {exp.coverage()*100:.1f}% of similar instances\")\n",
    "    \n",
    "    # Print important feature values \n",
    "    print(\"\\nKey feature values:\")\n",
    "    for rule in exp.names():\n",
    "        for i, feat in enumerate(feature_names):\n",
    "            if feat in rule:\n",
    "                print(f\"  ‚Ä¢ {feat}: {data_entry[i]}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "instance_prediction = model_predict_fn(np.expand_dims(data_entry, axis=0))[0] # Get prediction for this instance\n",
    "\n",
    "# Explain with optimized parameters\n",
    "print(\"Generating explanation (this may take some time)...\")\n",
    "exp = explainer.explain_instance(\n",
    "    data_entry, \n",
    "    model_predict_fn, \n",
    "    threshold=0.8,       # Lower precision requirement\n",
    "    delta=0.2,           # More lenient statistical guarantee\n",
    "    tau=0.2,             # More lenient precision constraint\n",
    "    batch_size=500,      # Larger batches for efficiency\n",
    "    max_anchor_size=3,   # Limit complexity\n",
    "    beam_size=2          # Smaller beam search\n",
    ")\n",
    "\n",
    "print_anchor_explanation(exp, instance_prediction, data_entry, features)\n",
    "\n",
    "# Display the original data\n",
    "print(\"\\nOriginal data:\")\n",
    "data_test.iloc[[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule means that if these conditions are met, the model‚Äôs fraud prediction is highly reliable. This explanation gives fraud analysts a clear rule they can understand and verify, rather than just a list of contributing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiCE üßä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DiCE (Diverse Counterfactual Explanations) [Paper, 2019](https://arxiv.org/abs/1905.07697)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Generates \"what-if\" scenarios showing the smallest changes needed to flip a model's prediction, making black box fraud detection decisions actionable and understandable.*\n",
    "\n",
    "##### DiCE Process in Fraud Detection:\n",
    "1. **Select:** Choose a flagged fraudulent transaction.\n",
    "2. **Generate:** Create realistic alternative versions base on an objective function that would be classified as legitimate.\n",
    "3. **Optimize:** Find the minimum changes needed to flip the prediction.\n",
    "4. **Compare:** Show what features would need to change (and by how much) to make the transaction legitimate.\n",
    "5. **Present:** Provide multiple diverse counterfactual explanations, not just one.\n",
    "\n",
    "##### Objective function for generating alternative versions:\n",
    "* **Proximity:** Ensures counterfactuals are close to the original transaction.\n",
    "* **Sparsity:** Minimizes the number of features that need to change.\n",
    "* **Diversity:** Provides multiple alternative paths to a different outcome.\n",
    "* **Feasibility:** Ensures changes are realistic (e.g., can't change transaction date to the future).\n",
    "\n",
    "##### Key Parameters:\n",
    "* **Proximity Weight:**\n",
    "  - **High weight:** Counterfactuals very similar to original transaction.\n",
    "  - **Low weight:** Allows more significant changes for greater diversity.\n",
    "* **Feature Weights:**\n",
    "  - Control which features are easier/harder to change (e.g., time is easier to change than location).\n",
    "\n",
    "\n",
    "\n",
    "[DiCE Code Tutorial from Microsoft](https://interpret.ml/DiCE/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Original Transaction Instance**\n",
    "\n",
    "```python\n",
    "transaction = {\n",
    "    'merchant': 'Rodriguez, Yost and Jenkins',\n",
    "    'category': 'misc_net',                   \n",
    "    'amt': 780.52,                            \n",
    "    'gender': 'M',                            \n",
    "    'lat': 42.5545,                           \n",
    "    'long': -90.3508,                         \n",
    "    'city_pop': 1306,                         \n",
    "    'unix_time': 1371853942,                  \n",
    "    'merch_lat': 42.461127,                   \n",
    "    'merch_long': -91.147148,                 \n",
    "    'age': 66,                                \n",
    "    'age_group': '60-69',                     \n",
    "    'dist': 66.097917,                        \n",
    "}\n",
    "```\n",
    "\n",
    "Model predicts this transaction as fraudulent with 0.9999497 probability\n",
    "\n",
    "##### **Step 1: Define the Objective Function**\n",
    "\n",
    "DiCE generates counterfactual examples by minimizing an objective function that balances several factors:\n",
    "\n",
    "$$\n",
    "\\text{Objective} = \\lambda_1 \\times \\underbrace{d(x,\\tilde{x})}_{\\text{Proximity}} + \\lambda_2 \\times \\underbrace{\\|x - \\tilde{x}\\|_0}_{\\text{Sparsity}} + \\lambda_3 \\times \\underbrace{D(\\tilde{x}_i, \\tilde{x}_j)}_{\\text{Diversity}} + \\lambda_4 \\times \\underbrace{\\mathcal{L}(f(\\tilde{x}), y_{\\text{target}})}_{\\text{Prediction Target}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Proximity:**  \n",
    "  Measures how similar the counterfactual $\\tilde{x}$ is to the original instance $x$ (using metrics such as L1 or L2 distance).  \n",
    "  *Example:* $L1([780.52, 66.09], [125.75, 3.21]) = |780.52 - 125.75| + |66.09 - 3.21| = 717.65$\n",
    "\n",
    "- **Sparsity:**  \n",
    "  Penalizes changing too many features (typically measured by the $L_0$ norm).\n",
    "\n",
    "- **Diversity:**  \n",
    "  Encourages generating counterfactuals that are different from one another by measuring distances in feature space.\n",
    "\n",
    "- **Prediction Target:**  \n",
    "  Ensures the counterfactual shifts the model's prediction toward the desired outcome, usually quantified by a loss term: $\\mathcal{L}(f(\\tilde{x}), y_{\\text{target}})$\n",
    "\n",
    "The coefficients $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, and $\\lambda_4$ balance these factors to find optimal counterfactuals.\n",
    "\n",
    "##### **Step 2: Generate Counterfactuals**\n",
    "\n",
    "Using the above objective function, **DiCE** generates candidate counterfactuals by slightly modifying feature values of the original transaction. These candidates are then evaluated with the objective function to select those that minimally differ from the original while achieving a different prediction.\n",
    "\n",
    "```python\n",
    "# Original fraudulent transaction (abbreviated)\n",
    "original = {\n",
    "    'merchant': 'Rodriguez, Yost and Jenkins', \n",
    "    'category': 'misc_net',\n",
    "    'amt': 780.52,\n",
    "    # ... other features\n",
    "    'dist': 66.10,\n",
    "    'age': 66,\n",
    "    'age_group': '60-69',\n",
    "    'city_pop': 1306\n",
    "}\n",
    "\n",
    "# Only showing changed features in each counterfactual\n",
    "counterfactuals = [\n",
    "    {   # CF1: Lower amount + different category\n",
    "        'category': 'grocery_pos',  # Changed from 'misc_net'\n",
    "        'amt': 125.75              # Changed from 780.52\n",
    "    },\n",
    "    {   # CF2: Known merchant + local transaction\n",
    "        'merchant': 'Walmart',      # Changed from law firm\n",
    "        'dist': 3.21               # Changed from 66.10 miles\n",
    "    },\n",
    "    {   # CF3: Urban location + younger customer\n",
    "        'city_pop': 2746388,        # Changed from 1,306\n",
    "        'age': 42,                  # Changed from 66\n",
    "        'age_group': '40-49'        # Changed from '60-69'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Model predictions for counterfactuals\n",
    "predictions = [0.08, 0.12, 0.15]  # All now classified as legitimate\n",
    "```\n",
    "\n",
    "##### **Step 3: Actionable Insights**\n",
    "\n",
    "| **Path** | **Key Changes** | **New Prediction** | **Legitimate Indicators** |\n",
    "|----------|----------------|-------------------|---------------------|\n",
    "| **Path 1** | *Category:* misc_net ‚Üí grocery_pos<br>*Amount:* $780.52 ‚Üí $125.75 | **0.08** (Legitimate) | In-person transaction<br>Lower, more typical amount |\n",
    "| **Path 2** | *Merchant:* Law firm ‚Üí Walmart<br>*Distance:* 66.10 ‚Üí 3.21 miles | **0.12** (Legitimate) | Well-known merchant<br>Local transaction |\n",
    "| **Path 3** | *City population:* 1,306 ‚Üí 2,746,388<br>*Age:* 66 ‚Üí 42 | **0.15** (Legitimate) | Major city location<br>Lower-risk age demographic |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify continuous features based on your dataset\n",
    "# continuous_features = data_test[features].select_dtypes(include=[float, int]).columns.to_list()\n",
    "\n",
    "# # Prepare training DataFrame (with original feature values) and add the outcome column\n",
    "# X_train_df = pd.DataFrame(X_train, columns=features)\n",
    "# X_train_df['is_fraud'] = y_train\n",
    "\n",
    "# # Create the DiCE Data object\n",
    "# dice_data = dice_ml.Data(\n",
    "#     dataframe=X_train_df,\n",
    "#     continuous_features=continuous_features,\n",
    "#     outcome_name='is_fraud'\n",
    "# )\n",
    "\n",
    "# # Define a custom prediction function that returns a tf.Tensor.\n",
    "# @tf.function  # Add TF function decorator for better performance\n",
    "# def dice_model_predict(x, training=False):\n",
    "#     # Convert to tensor once if it's not already\n",
    "#     if not isinstance(x, tf.Tensor):\n",
    "#         x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    \n",
    "#     # Reshape in a single operation\n",
    "#     x_reshaped = tf.expand_dims(x, axis=-1)\n",
    "    \n",
    "#     # Get predictions\n",
    "#     preds = model(x_reshaped, training=training)\n",
    "    \n",
    "#     # Create two-column probabilities\n",
    "#     return tf.concat([1 - preds, preds], axis=1)\n",
    "\n",
    "# # Create a DiCE Model object using the custom prediction function, with backend \"TF2\"\n",
    "# dice_model = dice_ml.Model(model=dice_model_predict, backend=\"TF2\")\n",
    "\n",
    "# # Initialize the DiCE Explainer (using the \"gradient\" method here)\n",
    "# explainer = dice_ml.Dice(dice_data, dice_model, method=\"gradient\")\n",
    "\n",
    "# # --- Select a Query Instance ---\n",
    "# # For example, choose one misclassified instance (ensure shape is (1, n_features))\n",
    "# idx = misclassified_indices[100]\n",
    "# data_entry = pd.DataFrame(\n",
    "#     X_test[[idx]],\n",
    "#     columns=features\n",
    "# )\n",
    "\n",
    "# print(f\"Generating counterfactuals for instance at index {idx}...\")\n",
    "\n",
    "# # --- Generate Counterfactuals ---\n",
    "# # 'total_CFs' defines how many counterfactual candidates to produce,\n",
    "# # and 'desired_class' being \"opposite\" instructs DiCE to flip the prediction.\n",
    "# dice_exp = explainer.generate_counterfactuals(\n",
    "#     data_entry, total_CFs=3, desired_class=\"opposite\", verbose =True\n",
    "# )\n",
    "\n",
    "# # Display the generated counterfactual explanations\n",
    "# print(dice_exp.cf_examples_list[0].final_cfs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define features\n",
    "# features = data_test.columns.drop('is_fraud').tolist()\n",
    "\n",
    "# # Define continuous and categorical features\n",
    "# continuous_features = data_test[features].select_dtypes(include=['number']).columns.tolist()\n",
    "# categorical_features = [f for f in features if f not in continuous_features]\n",
    "\n",
    "# # Choose an instance to explain\n",
    "# idx = misclassified_indices[100]\n",
    "\n",
    "# # Get counterfactuals\n",
    "# dice_explanations = get_dice_counterfactuals(\n",
    "#     model=model,\n",
    "#     X_train=X_train,\n",
    "#     y_train=y_train,\n",
    "#     X_test=X_test,\n",
    "#     features=features,\n",
    "#     idx=idx,\n",
    "#     misclassified_indices=misclassified_indices,\n",
    "#     scaler=transformations_test['scaler']  # Pass your scaler if needed\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeVbM_y2XCM8",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Single Feature Partial Dependence Plot\n",
    "\n",
    "[How to Build Shap Single Feature Partial Dependence Plot (PDP Plot)](https://www.youtube.com/watch?v=CgKyAlA-0wA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95709,
     "status": "aborted",
     "timestamp": 1742934046673,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "JPTXTAw9XCM8"
   },
   "outputs": [],
   "source": [
    "# Dependence plot for specific feature\n",
    "shap.dependence_plot(\"amt\", shap_values, X_test[from_idx:to_idx+1], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95712,
     "status": "aborted",
     "timestamp": 1742934046678,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "adUOWLEJXCM9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Other Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95720,
     "status": "aborted",
     "timestamp": 1742934046690,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "MOiCC9MlXCM9"
   },
   "outputs": [],
   "source": [
    "fraud_data = data_test[data_test['is_fraud'] == 1]\n",
    "non_fraud_data = data_test[data_test['is_fraud'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046696,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "dmMvfKQ6XCM9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Statistical analysis of top n features\n",
    "for feature in top_n_features:\n",
    "    plt.hist(fraud_data[feature].astype(int), alpha=0.5, label='Fraud', bins=30)\n",
    "    plt.hist(non_fraud_data[feature].astype(int), alpha=0.5, label='Non-Fraud', bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046705,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "_T-ckdALXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'amt'\n",
    "mean_value = data_test[feature].mean()\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature] > mean_value]) * 100 / len(data_test[data_test[feature] > mean_value])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046707,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "BShKKF_EXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'shopping_net'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95728,
     "status": "aborted",
     "timestamp": 1742934046710,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "aGTPMOVPXCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'grocery_pos'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046712,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "n57QfO-4XCM-"
   },
   "outputs": [],
   "source": [
    "feature = 'gas_transport'\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046714,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "KAdyBp_EXCM_"
   },
   "outputs": [],
   "source": [
    "feature = 'misc_net' # Miscellaneous online transactions\n",
    "\n",
    "fraud_ratio = len(fraud_data[fraud_data[feature]]) * 100 / len(data_test[data_test[feature]])\n",
    "legitimate_ratio = 100 - fraud_ratio\n",
    "\n",
    "plt.pie([fraud_ratio, legitimate_ratio],\n",
    "        labels=['Fraudulent', 'Legitimate'],\n",
    "        colors=['crimson', 'lightgreen'],\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "plt.title(f'Distribution of `{feature}` Transactions Above Mean of Dataset ({mean_value:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046716,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "UA4ZUPBfXCM_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "state_columns = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'FL', 'GA', 'HI', 'IA', \n",
    "                'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', \n",
    "                'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', \n",
    "                'OR', 'PA', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "\n",
    "states = []\n",
    "fraud_rates = []\n",
    "transaction_counts = []\n",
    "fraud_counts = []\n",
    "\n",
    "# Calculate metrics for each state\n",
    "for state in state_columns:\n",
    "    state_transactions = data_test[data_test[state] == 1]\n",
    "    total = len(state_transactions)\n",
    "    \n",
    "    if total > 0:\n",
    "        fraud_count = state_transactions['is_fraud'].sum()\n",
    "        fraud_rate = fraud_count / total\n",
    "        \n",
    "        states.append(state)\n",
    "        fraud_rates.append(fraud_rate)\n",
    "        transaction_counts.append(total)\n",
    "        fraud_counts.append(fraud_count)\n",
    "\n",
    "state_fraud_df = pd.DataFrame({\n",
    "    'State': states,\n",
    "    'Fraud_Rate': fraud_rates,\n",
    "    'Transaction_Count': transaction_counts,\n",
    "    'Fraud_Count': fraud_counts\n",
    "})\n",
    "\n",
    "# Sort by fraud rate descending\n",
    "state_fraud_df = state_fraud_df.sort_values('Fraud_Rate', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.barplot(x='Fraud_Rate', y='State', data=state_fraud_df, palette='viridis')\n",
    "\n",
    "plt.title('Fraud Rate by State', fontsize=16)\n",
    "plt.xlabel('Fraud Rate', fontsize=12)\n",
    "plt.ylabel('State', fontsize=12)\n",
    "\n",
    "for i, v in enumerate(state_fraud_df['Fraud_Rate']):\n",
    "    ax.text(v + 0.005, i, f\"{v:.2%}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a second visualization - bubble chart with fraud rates and transaction volume\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "scatter = plt.scatter(state_fraud_df['Transaction_Count'], \n",
    "                     state_fraud_df['Fraud_Rate'], \n",
    "                     s=state_fraud_df['Fraud_Count']*5,\n",
    "                     alpha=0.7,\n",
    "                     c=state_fraud_df['Fraud_Rate'],\n",
    "                     cmap='Reds')\n",
    "\n",
    "for _, row in state_fraud_df.head(5).iterrows():\n",
    "    plt.annotate(row['State'], \n",
    "                (row['Transaction_Count'], row['Fraud_Rate']),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold')\n",
    "\n",
    "plt.xscale('log')  # Use log scale for better visualization if counts vary widely\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Fraud Rate vs Transaction Volume by State', fontsize=16)\n",
    "plt.xlabel('Number of Transactions (log scale)', fontsize=12)\n",
    "plt.ylabel('Fraud Rate', fontsize=12)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Fraud Rate', rotation=270, labelpad=15)\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "legend1 = plt.legend(handles, labels, loc=\"upper left\", title=\"States\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My3D7T0iXCM_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Feature Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046718,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "ycm2Z80ZXCM_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def feature_ablation_study_global(model, X, y, from_idx, to_idx, selected_features, all_features):\n",
    "    global replacement_values, ablated_pred\n",
    "    base_pred = model.predict(X, verbose=0)\n",
    "    base_auc = roc_auc_score(y, base_pred)\n",
    "\n",
    "    # Precompute replacement values for each feature to avoid repeated computation.\n",
    "    replacement_values = {}\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        replacement_values[feature] = np.median(X[:, idx])\n",
    "\n",
    "    print(f\"Global (AUC) Feature Ablation Study from index [{from_idx}] to index [{to_idx}]:\")\n",
    "    test_set = X[from_idx:to_idx+1]\n",
    "    records = []\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        X_temp = test_set.copy()\n",
    "        X_temp[:, idx] = replacement_values[feature]\n",
    "\n",
    "        ablated_pred = model.predict(X_temp, verbose=0)\n",
    "        ablated_auc = roc_auc_score(y[from_idx:to_idx+1], ablated_pred)\n",
    "\n",
    "        impact = ((base_auc - ablated_auc) / base_auc) * 100\n",
    "\n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_auc': ablated_auc,\n",
    "            'impact_score': impact\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046720,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "4qWd05VDXCM_"
   },
   "outputs": [],
   "source": [
    "df_ablation_global = feature_ablation_study_global(model, X_test, y_test, from_idx, to_idx, top_n_features, features)\n",
    "df_ablation_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ2ugevLXCNA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Local Interterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95728,
     "status": "aborted",
     "timestamp": 1742934046721,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "9FPI9wDIXCNA"
   },
   "outputs": [],
   "source": [
    "def feature_ablation_single_entry(model, data_entry, selected_features, all_features):\n",
    "    baseline_values = np.zeros_like(data_entry)\n",
    "\n",
    "    original_prediction = model.predict(data_entry.reshape(1, -1), verbose=0)\n",
    "\n",
    "    print(f\"Local Feature Ablation Study: \")\n",
    "    records = []\n",
    "    for i, feature in enumerate(all_features):\n",
    "        ablated_entry = data_entry.copy()\n",
    "        ablated_entry[i] = baseline_values[i]\n",
    "\n",
    "        ablated_pred = model.predict(ablated_entry.reshape(1, -1), verbose=0) # Compute prediction on the ablated entry\n",
    "\n",
    "        impact = original_prediction - ablated_pred # Calculate the drop (or change) in prediction\n",
    "\n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_auc': ablated_pred.squeeze(),\n",
    "            'impact_score': impact.squeeze()\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "\n",
    "    return df_results, original_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95727,
     "status": "aborted",
     "timestamp": 1742934046722,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "12IJyedZXCNA"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "df_ablation_local, original_prob = feature_ablation_single_entry(model, X_test[idx], top_n_features, features)\n",
    "df_ablation_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046723,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "wJSufCzFXCNA"
   },
   "outputs": [],
   "source": [
    "# Plot feature contributions for a prediction\n",
    "shap.initjs()\n",
    "baseline = explainer.expected_value.numpy()\n",
    "\n",
    "shap.force_plot(baseline, shap_values[idx:idx+1], processed_data.loc[y_test.index].drop('is_fraud', axis=1).iloc[idx:idx+1], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046724,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "-rgfAovSXCNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046727,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "EagJ-FmRXCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046729,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "1m9NwVn1XCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95725,
     "status": "aborted",
     "timestamp": 1742934046730,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "Hjmu2QJUXCND"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046731,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "_yOnzYEPXCND"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742934290024,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "cyFcK8J4-jta",
    "outputId": "f72e1336-c4cc-4d93-b53a-119c2dbccafc"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95724,
     "status": "aborted",
     "timestamp": 1742934046736,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "fL6CdKzAbYtb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95723,
     "status": "aborted",
     "timestamp": 1742934046737,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "oJjgvocSXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95721,
     "status": "aborted",
     "timestamp": 1742934046738,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "jvL9JdiFXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046745,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "Wrm4cVrcXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95726,
     "status": "aborted",
     "timestamp": 1742934046746,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "NyoEOV_zXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95729,
     "status": "aborted",
     "timestamp": 1742934046751,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "xTQ6uswiXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8uIAv7vXCNE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 95737,
     "status": "aborted",
     "timestamp": 1742934046761,
     "user": {
      "displayName": "Minh Thong Lai",
      "userId": "05577324299846011042"
     },
     "user_tz": 0
    },
    "id": "91SSPOxpXCNE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtDHMDDWXCNE",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Not Active Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "def feature_ablation_study_local(model, X, y, index, selected_features, all_features):\n",
    "    global records, base_pred, base_diff, ablated_diff\n",
    "    base_pred = model.predict(X[index:index+1], verbose=0)\n",
    "    base_diff = y[index:index+1].to_numpy() - base_pred\n",
    "    \n",
    "    # Precompute replacement values for each feature to avoid repeated computation.\n",
    "    replacement_values = {}\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        replacement_values[feature] = np.median(X[:, idx])\n",
    "\n",
    "    print(f\"Local Feature Ablation Study of index [{index}]:\")\n",
    "    test_set = X[index:index+1]\n",
    "    records = []\n",
    "    for idx, feature in enumerate(all_features):\n",
    "        X_temp = test_set.copy()\n",
    "        X_temp[:, idx] = replacement_values[feature]\n",
    "        \n",
    "        ablated_pred = model.predict(X_temp, verbose=0)\n",
    "        ablated_diff = y[index:index+1].to_numpy() - ablated_pred\n",
    "        \n",
    "        impact = np.mean(base_diff - ablated_diff / base_diff)\n",
    "        \n",
    "        records.append({\n",
    "            'feature': feature,\n",
    "            'ablation_diff': np.mean(ablated_diff),\n",
    "            'impact_score': impact\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(records).sort_values(by='impact_score', ascending=False, key=abs).reset_index(drop=True)\n",
    "    df_results['ranking'] = df_results.index+1  # Ranking: 1 denotes the highest impact.\n",
    "    \n",
    "    df_results = df_results[df_results['feature'].isin(selected_features)].reset_index(drop=True)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "df_ablation_local = feature_ablation_study_local(model, X_test, y_test, 2, top_n_features, features)\n",
    "df_ablation_local\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpxZSOGLXCMk"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv_xai_fraud_detection",
   "language": "python",
   "name": ".venv_xai_fraud_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
